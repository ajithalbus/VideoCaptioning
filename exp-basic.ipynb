{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn as sk\n",
    "from sklearn import preprocessing as pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPER PARAMS\n",
    "GO='<go>'\n",
    "STOP='<stop>'\n",
    "PAD='<pad>'\n",
    "BATCH=20\n",
    "ATTEND=True\n",
    "BEAM=False\n",
    "BEAM_WIDTH=5\n",
    "EPOCHS=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREPROCESSING STARTS HERE\n",
    "def breaker(listofsentences,start=True):\n",
    "    '''returns a list of list of strings'''\n",
    "    if listofsentences==None:\n",
    "        return None\n",
    "    lst=[]\n",
    "    for i in listofsentences:\n",
    "        t=i.split()\n",
    "        if start:\n",
    "            t=[GO]+t+[PAD]\n",
    "        else:\n",
    "            t=[GO]+t+[STOP]\n",
    "        lst.append(t)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(data='train'):\n",
    "    COMBINED='./WeatherGov/'+data+'/'+data+'.combined'\n",
    "    SUMMARY='./WeatherGov/'+data+'/summaries.txt'\n",
    "    op_sec=None\n",
    "    with open(COMBINED) as f:\n",
    "        content = f.readlines()\n",
    "    ip_sec = [x.strip() for x in content]\n",
    "    if data!='test':\n",
    "        with open(SUMMARY) as f:\n",
    "            content = f.readlines()\n",
    "        op_sec = [x.strip() for x in content]\n",
    "        \n",
    "    return breaker(ip_sec,True),breaker(op_sec,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listofwords(data):\n",
    "    '''takes a list of sentences nd returns vocab'''\n",
    "    a=[]\n",
    "    for i in data:\n",
    "        for j in i:\n",
    "            if j not in a:\n",
    "                a.append(j)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ip_sec,training_op_sec=read('train')\n",
    "valid_ip_sec,valid_op_sec=read('dev')\n",
    "test_ip_sec,_=read('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_vocab=listofwords(training_ip_sec)\n",
    "op_vocab=listofwords(training_op_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_vocab_size,op_vocab_size=len(ip_vocab),len(op_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_ip_sec_len=[len(i) for i in training_ip_sec]\n",
    "training_op_sec_len=[len(i) for i in training_op_sec]\n",
    "valid_ip_sec_len=[len(i) for i in valid_ip_sec]\n",
    "valid_op_sec_len=[len(i) for i in valid_op_sec]\n",
    "test_ip_sec_len=[len(i) for i in test_ip_sec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_ip=pre.LabelEncoder()\n",
    "pre_ip.fit(ip_vocab)\n",
    "\n",
    "onehoter=np.identity(len(pre_ip.classes_))\n",
    "\n",
    "#word to int\n",
    "training_ip_sect=[pre_ip.transform(i) for i in training_ip_sec]\n",
    "valid_ip_sect=[pre_ip.transform(i) for i in valid_ip_sec]\n",
    "test_ip_sect=[pre_ip.transform(i) for i in test_ip_sec]\n",
    "\n",
    "#adding pads\n",
    "maxi=0\n",
    "for i in training_ip_sect:\n",
    "    maxi=max(maxi,len(i))\n",
    "training_ip_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_ip.transform([PAD])) for i in training_ip_sect]\n",
    "valid_ip_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_ip.transform([PAD])) for i in valid_ip_sect]\n",
    "test_ip_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_ip.transform([PAD])) for i in test_ip_sect]\n",
    "\n",
    "#making them onehot\n",
    "training_ip_sectt=[[onehoter[i] for i in j] for j in training_ip_sect0]\n",
    "valid_ip_sectt=[[onehoter[i] for i in j] for j in valid_ip_sect0]\n",
    "test_ip_sectt=[[onehoter[i] for i in j] for j in test_ip_sect0]\n",
    "\n",
    "\n",
    "#del training_ip_sec,valid_ip_sec,test_ip_sec\n",
    "#del training_ip_sect,valid_ip_sect,test_ip_sect\n",
    "#del training_ip_sect0,valid_ip_sect0,test_ip_sect0\n",
    "\n",
    "#OP sec\n",
    "pre_op=pre.LabelEncoder()\n",
    "pre_op.fit(op_vocab)\n",
    "\n",
    "onehoter2=np.identity(len(pre_op.classes_))\n",
    "\n",
    "#word to int\n",
    "training_op_sect=[pre_op.transform(i) for i in training_op_sec]\n",
    "valid_op_sect=[pre_op.transform(i) for i in valid_op_sec]\n",
    "\n",
    "#adding pad\n",
    "maxi=0\n",
    "for i in training_op_sect:\n",
    "    maxi=max(maxi,len(i))\n",
    "training_op_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_op.transform([STOP])) for i in training_op_sect]\n",
    "valid_op_sect0=[np.pad(i,(0,maxi-len(i)),'constant',constant_values=pre_op.transform([STOP])) for i in valid_op_sect]\n",
    "\n",
    "\n",
    "#int to onehot\n",
    "training_op_sectt=[[onehoter2[i] for i in j] for j in training_op_sect0]\n",
    "valid_op_sectt=[[onehoter2[i] for i in j] for j in valid_op_sect0]\n",
    "\n",
    "training_op_sectt_nopad=[[onehoter2[i] for i in j] for j in training_op_sect]\n",
    "valid_op_sectt_nopad=[[onehoter2[i] for i in j] for j in valid_op_sect]\n",
    "\n",
    "\n",
    "del training_op_sec,valid_op_sec\n",
    "del training_op_sect,valid_op_sect\n",
    "#del training_op_sect0,valid_op_sect0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxi=0\n",
    "for i in training_op_sect0:\n",
    "    maxi=max(maxi,len(i))\n",
    "#PREPROCESSING ENDS HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Graph building starts here\n",
    "embedding_size=256\n",
    "lstm_units=512\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#place holders\n",
    "source_seq = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "target_seq = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "source_seq_len = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "target_seq_len = tf.placeholder(shape=(None,), dtype=tf.int32)\n",
    "no_start_target_seq = tf.placeholder(shape=(None, None), dtype=tf.int32)\n",
    "trainer = tf.placeholder(shape=(None),dtype=tf.bool)\n",
    "batch_size = tf.placeholder(shape=(None),dtype=tf.int32)\n",
    "real_target_seq_len= tf.placeholder(shape=(None,), dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input and output embeddings\n",
    "embedding_matrix_encode = tf.get_variable(\n",
    "    name=\"embedding_matrix_en\",\n",
    "    shape=[ip_vocab_size, embedding_size],\n",
    "    dtype=tf.float32)\n",
    "embedding_matrix_decode = tf.get_variable(\n",
    "    name=\"embedding_matrix_de\",\n",
    "    shape=[op_vocab_size, embedding_size],\n",
    "    dtype=tf.float32)\n",
    "source_seq_embedded = tf.nn.embedding_lookup(embedding_matrix_encode, source_seq) \n",
    "decoder_input_embedded = tf.nn.embedding_lookup(embedding_matrix_decode, target_seq) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder\n",
    "(encoder_outputs_fw,encoder_outputs_bw) ,(encoder_state_fw,encoder_state_bw) = tf.nn.bidirectional_dynamic_rnn(\n",
    "    tf.contrib.rnn.LSTMCell(lstm_units),tf.contrib.rnn.LSTMCell(lstm_units),\n",
    "    source_seq_embedded,\n",
    "    sequence_length=source_seq_len,\n",
    "    dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention guys\n",
    "attention_states=tf.concat((encoder_outputs_fw,encoder_outputs_bw),1)\n",
    "attention_mechanism = tf.contrib.seq2seq.LuongAttention(lstm_units*2, attention_states,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat_3:0' shape=(?, ?, 512) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.concat((encoder_outputs_fw,encoder_outputs_bw),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat and Dropout\n",
    "encoder_final_state_c = tf.layers.dropout(inputs=tf.concat(\n",
    "    (encoder_state_fw.c, encoder_state_bw.c), 1),rate=0.4,training=trainer)\n",
    "\n",
    "encoder_final_state_h = tf.layers.dropout(inputs=tf.concat(\n",
    "    (encoder_state_fw.h, encoder_state_bw.h), 1),rate=0.4,training=trainer)\n",
    "\n",
    "encoder_final_state = tf.contrib.rnn.LSTMStateTuple(\n",
    "    c=encoder_final_state_c,\n",
    "    h=encoder_final_state_h\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Projection layer and decoder cell\n",
    "output_layer = tf.layers.Dense(op_vocab_size)\n",
    "\n",
    "decoder_cell=tf.contrib.rnn.LSTMCell(lstm_units*2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=<tf.Tensor 'dropout_2/cond/Merge:0' shape=(?, 1024) dtype=float32>, h=<tf.Tensor 'dropout_3/cond/Merge:0' shape=(?, 1024) dtype=float32>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_initial_state=encoder_final_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify decoder cell if attention\n",
    "if ATTEND:\n",
    "    decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "        decoder_cell, attention_mechanism,\n",
    "        attention_layer_size=lstm_units*2,alignment_history=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify decoder initial state if attention\n",
    "if ATTEND:\n",
    "    decoder_initial_state = decoder_cell.zero_state(BATCH, tf.float32).clone(cell_state=decoder_initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training helper and decoder\n",
    "helper = tf.contrib.seq2seq.TrainingHelper(decoder_input_embedded,target_seq_len)\n",
    "decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state=decoder_initial_state,output_layer=output_layer)#,output_layer=projection_layer)\n",
    "outputs, state, seq_len = tf.contrib.seq2seq.dynamic_decode(decoder)\n",
    "logits = outputs.rnn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference helper(greedy) and decoder\n",
    "helper2 = tf.contrib.seq2seq.GreedyEmbeddingHelper(embedding_matrix_decode,tf.fill([batch_size],\n",
    "                                                    np.int32(pre_op.transform([GO])[0])),\n",
    "                                                   np.int32(pre_op.transform([STOP])[0]))\n",
    "\n",
    "\n",
    "decoder2 = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper2, decoder_initial_state,output_layer=output_layer)#,output_layer=projection_layer)\n",
    "\n",
    "outputs, state, seq_len = tf.contrib.seq2seq.dynamic_decode(decoder2,maximum_iterations=maxi+10)\n",
    "\n",
    "translations_logits = outputs.rnn_output\n",
    "trs=outputs.sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beam search and decoder\n",
    "if BEAM:\n",
    "    decoder_initial_state_beam = tf.contrib.seq2seq.tile_batch(\n",
    "        decoder_initial_state, multiplier=BEAM_WIDTH)\n",
    "\n",
    "    # Define a beam-search decoder\n",
    "    decoder3 = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "            cell=decoder_cell,\n",
    "            embedding=embedding_matrix_decode,\n",
    "            start_tokens=tf.fill([batch_size],np.int32(pre_op.transform([GO])[0])),\n",
    "            end_token=np.int32(pre_op.transform([STOP])[0]),\n",
    "            initial_state=decoder_initial_state_beam,\n",
    "            beam_width=BEAM_WIDTH,\n",
    "            output_layer=output_layer,\n",
    "            length_penalty_weight=0.0)\n",
    "    outputs, state, seq_len = tf.contrib.seq2seq.dynamic_decode(decoder3,maximum_iterations=maxi+10)\n",
    "\n",
    "\n",
    "    trs_beam=outputs.predicted_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss and optimizer\n",
    "cross_entropy=tf.nn.sparse_softmax_cross_entropy_with_logits(labels=no_start_target_seq,logits=logits)\n",
    "\n",
    "target_weights = tf.sequence_mask(real_target_seq_len, target_seq_len[0], dtype=logits.dtype)\n",
    "\n",
    "loss=tf.reduce_sum(cross_entropy*target_weights)\n",
    "train = tf.train.AdamOptimizer().minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dont touch\n",
    "maxtlen=max(training_op_sec_len)\n",
    "maxvlen=max(valid_op_sec_len)\n",
    "t_newlen=[maxtlen-1 for i in range(len(training_op_sec_len))]\n",
    "v_newlen=[maxtlen-1 for i in range(len(valid_op_sec_len))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess=tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training starts here\n",
    "training_losses=[]\n",
    "valid_losses=[]\n",
    "for j in range(EPOCHS):\n",
    "    training_loss=0\n",
    "    for i in range(len(training_ip_sectt)/BATCH):\n",
    "        start=i*BATCH\n",
    "        stop=(i+1)*BATCH\n",
    "        \n",
    "        _,lost=sess.run([train,loss],feed_dict={source_seq:training_ip_sect0[start:stop],\n",
    "                                                target_seq:training_op_sect0[start:stop],\n",
    "                                              source_seq_len:training_ip_sec_len[start:stop],\n",
    "                                                target_seq_len:t_newlen[start:stop],\n",
    "                                                real_target_seq_len:training_op_sec_len[start:stop],\n",
    "                                                no_start_target_seq:np.array(training_op_sect0[start:stop])[:,1:],\n",
    "                                                batch_size:BATCH,\n",
    "                                                trainer:True\n",
    "                                                })\n",
    "        \n",
    "        training_loss+=lost\n",
    "        \n",
    "    #calculate t_loss\n",
    "    training_losses.append(training_loss/len(training_ip_sectt))\n",
    "    \n",
    "    #calculate v_loss\n",
    "    validation_loss=0\n",
    "    for k in range(len(valid_ip_sectt)/BATCH):\n",
    "        start=k*BATCH\n",
    "        stop=(k+1)*BATCH\n",
    "\n",
    "        lost=sess.run(loss,feed_dict={source_seq:valid_ip_sect0[start:stop],\n",
    "                                                target_seq:valid_op_sect0[start:stop],\n",
    "                                              source_seq_len:valid_ip_sec_len[start:stop],\n",
    "                                                target_seq_len:v_newlen[start:stop],\n",
    "                                                    real_target_seq_len:valid_op_sec_len[start:stop],\n",
    "                                                    no_start_target_seq:np.array(valid_op_sect0[start:stop])[:,1:],\n",
    "                                                    batch_size:BATCH,\n",
    "                                                    trainer:False})\n",
    "        validation_loss += lost\n",
    "        \n",
    "    valid_losses.append(validation_loss/len(valid_ip_sectt))\n",
    "    print \"Epoch:%d training loss%.4f: valid loss:%.4f\"% (j,training_losses[-1],valid_losses[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving loss values\n",
    "filer=open('./losses.txt','w+')\n",
    "for tl,vl in zip(training_losses,valid_losses):\n",
    "    filer.write(\"%f,%f\\n\" % (tl,vl))\n",
    "filer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize attention starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/44613173/how-to-visualize-attention-weights-from-attentionwrapper\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "def plot_attention(attention_map, input_tags = None, output_tags = None):    \n",
    "    \n",
    "    # input_tags - word representation of input sequence, use None to skip\n",
    "    # output_tags - word representation of output sequence, use None to skip\n",
    "    # i - index of input element in batch\n",
    "    \n",
    "    attn_len = len(attention_map)\n",
    "    shapeofmap=attention_map.shape\n",
    "    # Plot the attention_map\n",
    "    plt.clf()\n",
    "    f = plt.figure(figsize=(20, 30))\n",
    "    ax = f.add_subplot(1, 1, 1)\n",
    "\n",
    "    # Add image\n",
    "    i = ax.imshow(attention_map, cmap='Blues')\n",
    "\n",
    "    # Add colorbar\n",
    "    cbaxes = f.add_axes([0.2, 0.35, 0.6, 0.03])\n",
    "    cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')\n",
    "    cbar.ax.set_xlabel('Alpha value (Probability output of the \"softmax\")', labelpad=2)\n",
    "\n",
    "    # Add labels\n",
    "    ax.set_yticks(range(shapeofmap[0]))\n",
    "    if output_tags is not None:\n",
    "        ax.set_yticklabels(output_tags)\n",
    "\n",
    "    ax.set_xticks(range(shapeofmap[1]))\n",
    "    if input_tags is not None:\n",
    "        ax.set_xticklabels(input_tags, rotation=45)\n",
    "\n",
    "    ax.set_xlabel('Input Sequence')\n",
    "    ax.set_ylabel('Output Sequence')\n",
    "\n",
    "    # add grid and legend\n",
    "    ax.grid()\n",
    "    plt.savefig('x.eps',format='eps')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_images =tf.transpose(state.alignment_history.stack(), [1, 2, 0])\n",
    "start,stop=0,BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignments,y= sess.run([attention_images,outputs.sample_id], feed_dict={source_seq:valid_ip_sect0[start:stop],\n",
    "                                                target_seq:valid_op_sect0[start:stop],\n",
    "                                              source_seq_len:valid_ip_sec_len[start:stop],\n",
    "                                                target_seq_len:t_newlen[start:stop],\n",
    "                                                real_target_seq_len:valid_op_sec_len[start:stop],\n",
    "                                                no_start_target_seq:np.array(valid_op_sect0[start:stop])[:,1:],\n",
    "                                                batch_size:BATCH,\n",
    "                                                trainer:False\n",
    "                                                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imsave('t1.eps',alignments[0,:77,:19].T,format='eps')\n",
    "#print pre_op.inverse_transform(training_op_sect0[1])\n",
    "plot_attention(alignments[1,:78,:17].T,pre_ip.inverse_transform(valid_ip_sect0[1][:78]),\n",
    "               pre_op.inverse_transform(y[1,:17]))\n",
    "\n",
    "#visualize attention ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x,y=sess.run([translations_logits,trs],feed_dict={source_seq:training_ip_sect0[0:BATCH],\n",
    "                                               source_seq_len:training_ip_sec_len[0:BATCH],\n",
    "                                           trainer:False,batch_size:BATCH\n",
    "                                                })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pre_op.inverse_transform(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pre_op.inverse_transform(valid_op_sect0[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#generate test data summaries\n",
    "data=test_ip_sect0\n",
    "data_len=test_ip_sec_len\n",
    "gen_sum=[]\n",
    "for i in range(len(data)/BATCH):\n",
    "    start=i*BATCH\n",
    "    stop=(i+1)*BATCH\n",
    "    if BEAM:\n",
    "        load_trs=trs_beam\n",
    "    else:\n",
    "        load_trs=trs\n",
    "    y=sess.run(load_trs,feed_dict={source_seq:data[start:stop],\n",
    "                                               source_seq_len:data_len[start:stop],\n",
    "                                              batch_size:BATCH,\n",
    "                                                    trainer:False\n",
    "                                                })\n",
    "    if BEAM:\n",
    "        y=y[:,:,0]\n",
    "    for t in y:\n",
    "        gen_sum.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=test_ip_sect0\n",
    "data_len=test_ip_sec_len\n",
    "gen_sum=[]\n",
    "for i in range(len(data)/BATCH):\n",
    "    start=i*BATCH\n",
    "    stop=(i+1)*BATCH\n",
    "    if BEAM:\n",
    "        load_trs=trs_beam\n",
    "    else:\n",
    "        load_trs=trs\n",
    "    y=sess.run(load_trs,feed_dict={source_seq:data[start:stop],\n",
    "                                               source_seq_len:data_len[start:stop],\n",
    "                                              batch_size:BATCH,\n",
    "                                                    trainer:False\n",
    "                                                })\n",
    "    if BEAM:\n",
    "        y=y[:,:,0]\n",
    "    for t in y:\n",
    "        gen_sum.append(t)\n",
    "\n",
    "start=len(data)-BATCH\n",
    "stop=len(data)\n",
    "if BEAM:\n",
    "    load_trs=trs_beam\n",
    "else:\n",
    "    load_trs=trs\n",
    "y=sess.run(load_trs,feed_dict={source_seq:data[start:stop],\n",
    "                                            source_seq_len:data_len[start:stop],\n",
    "                                            batch_size:BATCH,\n",
    "                                                trainer:False\n",
    "                                            })\n",
    "if BEAM:\n",
    "    y=y[:,:,0]\n",
    "\n",
    "y=y[-(len(data)-len(gen_sum)):]\n",
    "for t in y:\n",
    "    gen_sum.append(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing summaries\n",
    "summs=[]\n",
    "for i in gen_sum:\n",
    "    summ=''\n",
    "    for j in i:\n",
    "        \n",
    "        if j!=205:\n",
    "            summ = summ+' '+pre_op.inverse_transform(j)\n",
    "    summs.append(summ[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save summaries\n",
    "filer=open('./sum.txt','w+')\n",
    "for item in summs:\n",
    "    filer.write(\"%s\\n\" % item)\n",
    "filer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./new/max-1000'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save model\n",
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './model',global_step=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#restore model\n",
    "saver = tf.train.import_meta_graph('./model-1000.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('./'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([150, 185, 187, 103, 176,  62, 175,  82, 174,  94, 188, 187, 103,\n",
       "       176,  33, 175,  57, 174,  85, 190, 187, 103, 176, 101, 175, 127,\n",
       "       174,  37, 179,  34, 189, 187, 103, 177, 165, 173, 187, 103, 176,\n",
       "        33, 175,  33, 174,  33, 182, 187, 103, 178,  91, 182, 187, 104,\n",
       "       178,  35, 182, 187, 102, 178,  61, 182, 187, 139, 178,  91, 182,\n",
       "       187,  43, 178,  91, 180, 187, 103, 176,  33, 175,  53, 174,  41,\n",
       "       151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151,\n",
       "       151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151,\n",
       "       151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151, 151,\n",
       "       151, 151, 151, 151, 151, 151, 151])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ip_sect0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
